\chapter{Context}
This work is a bachelor degree of a computer engineering degree, specialization in Computing.
The degree is done in the Facultad d'Informatica de Barcelona (FIB) of Universitat Polit√†cnica de Catalunya (UPC) and is directed by Gerard Escudero Bakx and supervised by Edelmira Pasarella Sanchez.

This project is based on the master thesis made by Juan Pablo Royo Sales in 2021 \cite{juan_pablo_royo_sales_incremental_2021}. that was also supervised by Edelmira.
Juan Pablo master thesis is about an implementation of an algorithm and my objective is to improve algorithm he implemented.
And that is why, in this first section, I would like to introduce some concepts that are important to understand the basics of the Juan Pablo work and also to understand the concepts that I will be using in this project.
Therefore, here only will be explained the concepts that are important to my work, summarizing and adding some notes to Juan Pablo's work.
If you are interested in more detailed explanation of the concepts, you can read Juan Pablo's work.

\section{Introduction of concepts}
Nowadays, the amount of data that has been generated is huge and more important, it is growing every day.
Sensors, social networks, and other sources generate data that needs to be processed and analyzed to extract useful information.
The main problem is that, when we try to process this data, we can not use the traditional methods that we use with small amount of data, because the time and resources needed are too high.
To solve this problem, we need to develop new algorithms and techniques that can deal with. \\
\subsection*{Streaming}
In addition to having all that data, the amount of data normally it can not be stored and it comes in what is called a data stream.
A data stream is a sequence of data that made available over time, meaning that we can not store all the data and we need to compute it in time.
For example, a sensor of temperature that sends the temperature every second, a traffic camera that registers all car plates that pass in front of it or just a social network that generates a huge amount of data every second.\\
This kind of data mentioned before needs to be processed and sometimes the data never ends, so we can not wait to finish to give a result and we must give results along the way.
\subsection*{Incremental algorithms}
Here is where incremental algorithms come in.\cite{sharp_incremental_2007}
Incremental algorithms give us the ability to obtain results from subsets of data and then update the results before finishing the whole data.
This is very useful, because some problems do not need to be solved with all the data or maybe we are not interested in the final result.
Recovering a previous example, if we are interested in which models of cars drive in certain road, we can use the camera that registers the car plates to get the answer.
It is stupid to wait until the end of data to give the answer (also because it never ends), so we can give a result when we check it.
In conclusion, incremental algorithms could be a good approach to solve some problems.
\subsection*{Paralelism}
One of the most important techniques for dealing with time problem is parallel computing or parallelism.
Parallelism allows us to divide the work and process it in different machines concurrently, reducing the time needed to process the data.
When we try to fight against this huge data problems, we must find a solution that can be parallelized given that modern machines have multiple cores and we can use them to process the data.\\
If we put together streaming and parallelism, it can be distinguished two computational models:
\begin{itemize}
    \item \textbf{Data Parallelism} \\ 
        This model splits the data and processes it in parallel.
        All the computations that perform some action over a subset of data, do not have any dependency with other parallel computations.
        This model has the advantage that it can implement stateless algorithms, allowing to split and process the data into different machines without contextual information.
        Nonetheless, this model has the disadvantage that when we need to be aware of the context, it is penalized.
    \item \textbf{Pipeline Parallelism:} \\
        This model splits the computation in different stages and each stage takes the result of the previous stage to make the computation.
        The parallelization is done by parallelizing the stages.
        The main advantage is that stages are non-blocking, meaning that we do not need to process all data to execute the next stage.
        This allows us to make incremental algorithms.
        In spite of that, the main disadvantage is that one stage could be the bottleneck of the pipeline and delaying all the process.
    \end{itemize}
\subsection*{Dynamic Pipeline Paradigm}
Now that we talked about these 3 concepts: incremental algorithms, streaming and parallelism, I can introduce the next concept that I am going to be working in this project. \\
The dynamic pipeline Paradigm is a Pipeline Parallelism model "\textit{based on a one-dimensional and unidirectional chain of stages connected by means of channels synchronized by data availability}". \cite*[][Page 9, 2.2]{juan_pablo_royo_sales_incremental_2021}
This chain is called Dynamic Pipeline and it can grow and shrink with the continuous arrival of data.
So we can use a data stream to feed the pipeline and it will process the data growing and shrinking the as needed.
With this paradigm we can implement incremental algorithms and process the data in parallel easily.
\section{Problem to be solved}
In his work, Juan Pablo implemented an incremental algorithm using dynamic pipeline paradigm to resolve a graph problem: finding bitriangles in bipartite graphs.
He decided to implement the algorithm using the functional programming language Haskell, and because of the no existence of one, he created a framework to implement the dynamic pipeline paradigm.
Here is where my project comes in, I will use his framework to implement an easier problem to understand the dynamic pipeline paradigm and then I will try to improve the original algorithm that he made.

As said before, the algorithm to improve finds bitriangles in bipartite graphs and here i will not explain the problem since I consider that it is a little difficult and the nature of the problem is not decisive enough for my resolution.
If you are interested in the problem, you can read Juan Pablo's work \cite[][Page 5, 1.1]{juan_pablo_royo_sales_incremental_2021}.

The easier problem that I chose for learning about the Haskell framework is the word counting problem.
As easy as it sounds, the problem does not need further explanation: we just need to count the number of occurrences of each word in a dataset.
\section{Stakeholders}
This project is co-supervised by Edelmira Pasarella Sanchez, who is a researcher and supervised Juan Pablo work.
She and her team developed the algorithm so they are the main stakeholders of this project.
Also the director of this project, Gerard Escudero Bakx, is the one who proposed me to do this project because he was interested in the Haskell framework made by Juan Pablo.
So Gerard is also a stakeholder of this project.

Apart from these direct stakeholders, this project will help to add knowledge to the community about the dynamic pipeline paradigm code examples and more important, will add more code and knowledge to the world about the Haskell framework of Juan Pablo.

\chapter{Justification}
Well first we need to ask ourselves if is it worth or necessary to improve the algorithm.
As I do not really know well about the algorithm, I talked with Edelmira and discuss some points where we can improve the algorithm.
Their team has done some improvements since Juan Pablo's work, and also tell me about some weak points of the algorithm.
So yes, I think that is worth to improve the algorithm.\\

With my actual knowledge of the algorithm, I can not tell if it is better to improve the algorithm or to make a new one.
But another time Edelmira told me that the algorithm was good and that it was worth to improve it.

Another point to take into account if it is worth to use the Haskell framework made by Juan Pablo.
This may not be clear now, because this framework is unique and there are no other examples of it.
This work can be very useful for testing the framework and extracting a conclusion about this.

\chapter{Scope}
\section{Objectives}
This project has the following main objectives:
\begin{list}{-}{}
    \item \textbf{Word counting problem:} The first goal of this project is to make an implementation, using the Haskell framework for dinamic pipeline paradigm, of the word counting problem. 
    \item \textbf{Improve Juan Pablo's algorithm:} The second goal is to improve the implementation of the algorithm mentoined before.
\end{list}
Apart from these main objectives, I have some sub-objectives that I would like to achieve at the end of the project:
\begin{list}{-}{}
    \item Learn about the dynamic pipeline paradigm.
    \item Improve my Haskell skills, from my actual basic level to a competent level.
    \item Learn about the inner workings of the Haskell programming language.
    \item Acquire knowledge of how to do a bachelor degree project for future projects like master thesis.
\end{list}
\section{Requirements}
For the correct development and validity of my final result, it is necessary that my final implementation solves all cases correctly and more efficiently on average than the implementation from which we started.
It is also important to follow a correct and modular structure to facilitate possible modification.
My optimization has to be in line with known improvements and must be correctly validated.
My solution also has to be understandable and adaptable, in order to improve its reuse and sharing.
As for the code, I have to make sure it is well documented.
Finally, it is necessary that all Haskell functionalities used are updated and supported
\section{Potential obstacles and risks}
Like all projects, you always have to take into account the potential obstacles and risks that can appear.
Here are the principal ones that I have identified:
\subsection{Time Limit}
I will say that this is one of the most important risks, because as there is a deadline to finish the project, an inconvenience can make to not finish the project.
Despite this, I this that I have done a good objective planning and I will be able to redirect the project if any problem appears.
\subsection{Dynamic pipeline framework}
I did not use and examined the framework and a problem that worries me is that the framework has some bugs or is not well implemented.
This can potentially make me lose a lot of time but as I check, Pablo did a good work documenting it so I trust that I will not have a lot of problems.

\chapter{Methodology and rigor}
\section{Methodology}
For this project, I will following a methodology based on the Scrum methodology, as I have always work with it and I have had good results.
Originaly, Scrum \cite{noauthor_what_nodate} was designed for team work, but I will going to take the idea and adapt it to my project, that only I do it individually. \\

Scrum is a methodology that is based on the iterative and incremental development of the project, where the project is divided into small tasks that are done in a short period of time called sprint.
This sprints have 3 main phases: planning, development and review.
Taking this concepts into acount, I will divide the project into tasks and I will be following 1 week sprints.
It will help me keep good control of the pace of the project and not fall asleep or fall behind.
Apart from the experience using this methodology, another of the main reasons for using it is the potential to redirect the project in case of possible problems, as I consider crucial.
\section{Project monitoring and validation}
As good programming practices, I will be using git \cite{noauthor_git_nodate} to control the versions of the code.
This will allow me to have a backup of the code and a practical way to a acces to previous versions.
Also, I will be using Trello as a task manager, to keep track of the tasks and the progress of the project.
Trello is very useful and combines very well with the methodology that I have chosen, since I will be able to have a record of the tasks to be done and the tasks completed. \\

To validate all about the Haskell work, I will be asking and following Gerard advice. 
Since Gerard has a great level of Haskell language, he will be able to help me validate that I follow a good structure and use of Haskell.
Finally, facing the end of the project, Edelmira will help me with the correction and validation of the improvement and implementation of the algorithm.





